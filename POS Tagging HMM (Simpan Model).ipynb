{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kalimat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bocah/NOUN kuwi/DET seneng/VERB nggambar/VERB ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ing/ADP wayah/NOUN sore/NOUN biasane/ADV Siti/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mbakyune/_ Mbakyu/NOUN e/PRON seneng/VERB ngla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ibu/NOUN kuwi/DET ngendikakake/VERB yen/SCONJ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nonton/VERB tivi/NOUN bisa/AUX njembarake/VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>/PUNCT Sesuk/ADV aku/PRON pengen/VERB ngerti/V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>/PUNCT Dhik/PROPN ,/PUNCT kok/ADV sajak/SCONJ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>Bocah/NOUN enom/ADJ saiki/ADV racake/ADV akeh/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>Lha/INTJ ning/PART nek/SCONJ bapak/NOUN biyen/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>Sampun/AUX damel/VERB Ibu/NOUN kuwatir/ADJ ./P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               kalimat\n",
       "0    Bocah/NOUN kuwi/DET seneng/VERB nggambar/VERB ...\n",
       "1    Ing/ADP wayah/NOUN sore/NOUN biasane/ADV Siti/...\n",
       "2    Mbakyune/_ Mbakyu/NOUN e/PRON seneng/VERB ngla...\n",
       "3    Ibu/NOUN kuwi/DET ngendikakake/VERB yen/SCONJ ...\n",
       "4    Nonton/VERB tivi/NOUN bisa/AUX njembarake/VERB...\n",
       "..                                                 ...\n",
       "834  /PUNCT Sesuk/ADV aku/PRON pengen/VERB ngerti/V...\n",
       "835  /PUNCT Dhik/PROPN ,/PUNCT kok/ADV sajak/SCONJ ...\n",
       "836  Bocah/NOUN enom/ADJ saiki/ADV racake/ADV akeh/...\n",
       "837  Lha/INTJ ning/PART nek/SCONJ bapak/NOUN biyen/...\n",
       "838  Sampun/AUX damel/VERB Ibu/NOUN kuwatir/ADJ ./P...\n",
       "\n",
       "[839 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('output_with_pos.txt', delimiter='\\t', header=None, names=['kalimat'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kalimat</th>\n",
       "      <th>kata_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bocah/NOUN kuwi/DET seneng/VERB nggambar/VERB ...</td>\n",
       "      <td>[(Bocah, NOUN), (kuwi, DET), (seneng, VERB), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ing/ADP wayah/NOUN sore/NOUN biasane/ADV Siti/...</td>\n",
       "      <td>[(Ing, ADP), (wayah, NOUN), (sore, NOUN), (bia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mbakyune/_ Mbakyu/NOUN e/PRON seneng/VERB ngla...</td>\n",
       "      <td>[(Mbakyune, _), (Mbakyu, NOUN), (e, PRON), (se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ibu/NOUN kuwi/DET ngendikakake/VERB yen/SCONJ ...</td>\n",
       "      <td>[(Ibu, NOUN), (kuwi, DET), (ngendikakake, VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nonton/VERB tivi/NOUN bisa/AUX njembarake/VERB...</td>\n",
       "      <td>[(Nonton, VERB), (tivi, NOUN), (bisa, AUX), (n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>/PUNCT Sesuk/ADV aku/PRON pengen/VERB ngerti/V...</td>\n",
       "      <td>[(, PUNCT), (Sesuk, ADV), (aku, PRON), (pengen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>/PUNCT Dhik/PROPN ,/PUNCT kok/ADV sajak/SCONJ ...</td>\n",
       "      <td>[(, PUNCT), (Dhik, PROPN), (,, PUNCT), (kok, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>Bocah/NOUN enom/ADJ saiki/ADV racake/ADV akeh/...</td>\n",
       "      <td>[(Bocah, NOUN), (enom, ADJ), (saiki, ADV), (ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>Lha/INTJ ning/PART nek/SCONJ bapak/NOUN biyen/...</td>\n",
       "      <td>[(Lha, INTJ), (ning, PART), (nek, SCONJ), (bap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>Sampun/AUX damel/VERB Ibu/NOUN kuwatir/ADJ ./P...</td>\n",
       "      <td>[(Sampun, AUX), (damel, VERB), (Ibu, NOUN), (k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               kalimat  \\\n",
       "0    Bocah/NOUN kuwi/DET seneng/VERB nggambar/VERB ...   \n",
       "1    Ing/ADP wayah/NOUN sore/NOUN biasane/ADV Siti/...   \n",
       "2    Mbakyune/_ Mbakyu/NOUN e/PRON seneng/VERB ngla...   \n",
       "3    Ibu/NOUN kuwi/DET ngendikakake/VERB yen/SCONJ ...   \n",
       "4    Nonton/VERB tivi/NOUN bisa/AUX njembarake/VERB...   \n",
       "..                                                 ...   \n",
       "834  /PUNCT Sesuk/ADV aku/PRON pengen/VERB ngerti/V...   \n",
       "835  /PUNCT Dhik/PROPN ,/PUNCT kok/ADV sajak/SCONJ ...   \n",
       "836  Bocah/NOUN enom/ADJ saiki/ADV racake/ADV akeh/...   \n",
       "837  Lha/INTJ ning/PART nek/SCONJ bapak/NOUN biyen/...   \n",
       "838  Sampun/AUX damel/VERB Ibu/NOUN kuwatir/ADJ ./P...   \n",
       "\n",
       "                                              kata_tag  \n",
       "0    [(Bocah, NOUN), (kuwi, DET), (seneng, VERB), (...  \n",
       "1    [(Ing, ADP), (wayah, NOUN), (sore, NOUN), (bia...  \n",
       "2    [(Mbakyune, _), (Mbakyu, NOUN), (e, PRON), (se...  \n",
       "3    [(Ibu, NOUN), (kuwi, DET), (ngendikakake, VERB...  \n",
       "4    [(Nonton, VERB), (tivi, NOUN), (bisa, AUX), (n...  \n",
       "..                                                 ...  \n",
       "834  [(, PUNCT), (Sesuk, ADV), (aku, PRON), (pengen...  \n",
       "835  [(, PUNCT), (Dhik, PROPN), (,, PUNCT), (kok, A...  \n",
       "836  [(Bocah, NOUN), (enom, ADJ), (saiki, ADV), (ra...  \n",
       "837  [(Lha, INTJ), (ning, PART), (nek, SCONJ), (bap...  \n",
       "838  [(Sampun, AUX), (damel, VERB), (Ibu, NOUN), (k...  \n",
       "\n",
       "[839 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fungsi untuk memisahkan setiap kalimat menjadi kata dan tag\n",
    "def split_words_tags(sentence):\n",
    "    words_tags = sentence.split()\n",
    "    return [tuple(word_tag.rsplit('/', 1)) for word_tag in words_tags]\n",
    "\n",
    "data['kata_tag'] = data['kalimat'].apply(split_words_tags)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kalimat</th>\n",
       "      <th>kata_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bocah/NOUN kuwi/DET seneng/VERB nggambar/VERB ...</td>\n",
       "      <td>[(bocah, NOUN), (kuwi, DET), (seneng, VERB), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ing/ADP wayah/NOUN sore/NOUN biasane/ADV Siti/...</td>\n",
       "      <td>[(ing, ADP), (wayah, NOUN), (sore, NOUN), (bia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mbakyune/_ Mbakyu/NOUN e/PRON seneng/VERB ngla...</td>\n",
       "      <td>[(mbakyune, _), (mbakyu, NOUN), (e, PRON), (se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ibu/NOUN kuwi/DET ngendikakake/VERB yen/SCONJ ...</td>\n",
       "      <td>[(ibu, NOUN), (kuwi, DET), (ngendikakake, VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nonton/VERB tivi/NOUN bisa/AUX njembarake/VERB...</td>\n",
       "      <td>[(nonton, VERB), (tivi, NOUN), (bisa, AUX), (n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>/PUNCT Sesuk/ADV aku/PRON pengen/VERB ngerti/V...</td>\n",
       "      <td>[(, PUNCT), (sesuk, ADV), (aku, PRON), (pengen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>/PUNCT Dhik/PROPN ,/PUNCT kok/ADV sajak/SCONJ ...</td>\n",
       "      <td>[(, PUNCT), (dhik, PROPN), (,, PUNCT), (kok, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>Bocah/NOUN enom/ADJ saiki/ADV racake/ADV akeh/...</td>\n",
       "      <td>[(bocah, NOUN), (enom, ADJ), (saiki, ADV), (ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>Lha/INTJ ning/PART nek/SCONJ bapak/NOUN biyen/...</td>\n",
       "      <td>[(lha, INTJ), (ning, PART), (nek, SCONJ), (bap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>Sampun/AUX damel/VERB Ibu/NOUN kuwatir/ADJ ./P...</td>\n",
       "      <td>[(sampun, AUX), (damel, VERB), (ibu, NOUN), (k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               kalimat  \\\n",
       "0    Bocah/NOUN kuwi/DET seneng/VERB nggambar/VERB ...   \n",
       "1    Ing/ADP wayah/NOUN sore/NOUN biasane/ADV Siti/...   \n",
       "2    Mbakyune/_ Mbakyu/NOUN e/PRON seneng/VERB ngla...   \n",
       "3    Ibu/NOUN kuwi/DET ngendikakake/VERB yen/SCONJ ...   \n",
       "4    Nonton/VERB tivi/NOUN bisa/AUX njembarake/VERB...   \n",
       "..                                                 ...   \n",
       "834  /PUNCT Sesuk/ADV aku/PRON pengen/VERB ngerti/V...   \n",
       "835  /PUNCT Dhik/PROPN ,/PUNCT kok/ADV sajak/SCONJ ...   \n",
       "836  Bocah/NOUN enom/ADJ saiki/ADV racake/ADV akeh/...   \n",
       "837  Lha/INTJ ning/PART nek/SCONJ bapak/NOUN biyen/...   \n",
       "838  Sampun/AUX damel/VERB Ibu/NOUN kuwatir/ADJ ./P...   \n",
       "\n",
       "                                              kata_tag  \n",
       "0    [(bocah, NOUN), (kuwi, DET), (seneng, VERB), (...  \n",
       "1    [(ing, ADP), (wayah, NOUN), (sore, NOUN), (bia...  \n",
       "2    [(mbakyune, _), (mbakyu, NOUN), (e, PRON), (se...  \n",
       "3    [(ibu, NOUN), (kuwi, DET), (ngendikakake, VERB...  \n",
       "4    [(nonton, VERB), (tivi, NOUN), (bisa, AUX), (n...  \n",
       "..                                                 ...  \n",
       "834  [(, PUNCT), (sesuk, ADV), (aku, PRON), (pengen...  \n",
       "835  [(, PUNCT), (dhik, PROPN), (,, PUNCT), (kok, A...  \n",
       "836  [(bocah, NOUN), (enom, ADJ), (saiki, ADV), (ra...  \n",
       "837  [(lha, INTJ), (ning, PART), (nek, SCONJ), (bap...  \n",
       "838  [(sampun, AUX), (damel, VERB), (ibu, NOUN), (k...  \n",
       "\n",
       "[839 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case Folding (Lowercase)\n",
    "def case_folding(word_tag_list):\n",
    "    return [(word.lower(), tag) for word, tag in word_tag_list]\n",
    "\n",
    "data['kata_tag'] = data['kata_tag'].apply(case_folding)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah kata dan tag yang berupa '_': 595\n"
     ]
    }
   ],
   "source": [
    "underscore = data['kata_tag'].apply(lambda kalimat: sum(1 for kata, tag in kalimat if kata == '_' or tag == '_')).sum()\n",
    "print(f\"Jumlah kata dan tag yang berupa '_': {underscore}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah kata dan tag yang berupa '_': 0\n"
     ]
    }
   ],
   "source": [
    "# Menghapus kata dan tag yang berupa '_'\n",
    "data['kata_tag'] = data['kata_tag'].apply(lambda kalimat: [(kata, tag) for kata, tag in kalimat if kata != '_' and tag != '_'])\n",
    "underscore = data['kata_tag'].apply(lambda kalimat: sum(1 for kata, tag in kalimat if kata == '_' or tag == '_')).sum()\n",
    "print(f\"Jumlah kata dan tag yang berupa '_': {underscore}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kalimat</th>\n",
       "      <th>kata_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bocah kuwi seneng nggambar sesawangan sing asri .</td>\n",
       "      <td>[(bocah, NOUN), (kuwi, DET), (seneng, VERB), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ing wayah sore biasane siti sinau , kangmas e ...</td>\n",
       "      <td>[(ing, ADP), (wayah, NOUN), (sore, NOUN), (bia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mbakyu e seneng nglangi , nanging adhi e ora s...</td>\n",
       "      <td>[(mbakyu, NOUN), (e, PRON), (seneng, VERB), (n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ibu kuwi ngendikakake yen putra e mbarep wis n...</td>\n",
       "      <td>[(ibu, NOUN), (kuwi, DET), (ngendikakake, VERB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nonton tivi bisa njembarake kawruh , nanging u...</td>\n",
       "      <td>[(nonton, VERB), (tivi, NOUN), (bisa, AUX), (n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>kanthi rasa bungah , dak rangkul wong tuwa ku ...</td>\n",
       "      <td>[(kanthi, ADP), (rasa, NOUN), (bungah, ADJ), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>kados dereng saged nampi kasunyatan niki , ” ...</td>\n",
       "      <td>[(, PUNCT), (kados, ADV), (dereng, ADV), (sage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>bocah enom saiki racake akeh sing lali karo ka...</td>\n",
       "      <td>[(bocah, NOUN), (enom, ADJ), (saiki, ADV), (ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>lha ning nek bapak biyen sida lunga , ora ana ...</td>\n",
       "      <td>[(lha, INTJ), (ning, PART), (nek, SCONJ), (bap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>sampun damel ibu kuwatir .</td>\n",
       "      <td>[(sampun, AUX), (damel, VERB), (ibu, NOUN), (k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1020 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                kalimat  \\\n",
       "0     bocah kuwi seneng nggambar sesawangan sing asri .   \n",
       "1     ing wayah sore biasane siti sinau , kangmas e ...   \n",
       "2     mbakyu e seneng nglangi , nanging adhi e ora s...   \n",
       "3     ibu kuwi ngendikakake yen putra e mbarep wis n...   \n",
       "4     nonton tivi bisa njembarake kawruh , nanging u...   \n",
       "...                                                 ...   \n",
       "1015  kanthi rasa bungah , dak rangkul wong tuwa ku ...   \n",
       "1016   kados dereng saged nampi kasunyatan niki , ” ...   \n",
       "1017  bocah enom saiki racake akeh sing lali karo ka...   \n",
       "1018  lha ning nek bapak biyen sida lunga , ora ana ...   \n",
       "1019                         sampun damel ibu kuwatir .   \n",
       "\n",
       "                                               kata_tag  \n",
       "0     [(bocah, NOUN), (kuwi, DET), (seneng, VERB), (...  \n",
       "1     [(ing, ADP), (wayah, NOUN), (sore, NOUN), (bia...  \n",
       "2     [(mbakyu, NOUN), (e, PRON), (seneng, VERB), (n...  \n",
       "3     [(ibu, NOUN), (kuwi, DET), (ngendikakake, VERB...  \n",
       "4     [(nonton, VERB), (tivi, NOUN), (bisa, AUX), (n...  \n",
       "...                                                 ...  \n",
       "1015  [(kanthi, ADP), (rasa, NOUN), (bungah, ADJ), (...  \n",
       "1016  [(, PUNCT), (kados, ADV), (dereng, ADV), (sage...  \n",
       "1017  [(bocah, NOUN), (enom, ADJ), (saiki, ADV), (ra...  \n",
       "1018  [(lha, INTJ), (ning, PART), (nek, SCONJ), (bap...  \n",
       "1019  [(sampun, AUX), (damel, VERB), (ibu, NOUN), (k...  \n",
       "\n",
       "[1020 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fungsi untuk memisahkan kalimat berdasarkan kata PUNCT\n",
    "def split_after_punct(kalimat):\n",
    "    result = []\n",
    "    temp = []\n",
    "    for kata, tag in kalimat:\n",
    "        temp.append((kata, tag))\n",
    "        if tag == 'PUNCT' and kata == '.':\n",
    "            result.append(temp)\n",
    "            temp = []\n",
    "    if temp:  # Menambahkan kalimat terakhir jika ada\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "# Menerapkan fungsi split_after_punct\n",
    "data['kalimat_split'] = data['kata_tag'].apply(split_after_punct)\n",
    "\n",
    "# Membuat DataFrame baru dengan setiap kalimat terpisah\n",
    "split_rows = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    for kalimat in row['kalimat_split']:\n",
    "        split_rows.append({'kalimat': ' '.join(kata for kata, tag in kalimat), 'kata_tag': kalimat})\n",
    "\n",
    "# Membuat DataFrame baru dari hasil split\n",
    "split_df = pd.DataFrame(split_rows)\n",
    "\n",
    "split_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [(bocah, NOUN), (kuwi, DET), (seneng, VERB), (...\n",
      "1       [(ing, ADP), (wayah, NOUN), (sore, NOUN), (bia...\n",
      "2       [(mbakyu, NOUN), (e, PRON), (seneng, VERB), (n...\n",
      "3       [(ibu, NOUN), (kuwi, DET), (ngendikakake, VERB...\n",
      "4       [(nonton, VERB), (tivi, NOUN), (bisa, AUX), (n...\n",
      "                              ...                        \n",
      "1015    [(kanthi, ADP), (rasa, NOUN), (bungah, ADJ), (...\n",
      "1016    [(, PUNCT), (kados, ADV), (dereng, ADV), (sage...\n",
      "1017    [(bocah, NOUN), (enom, ADJ), (saiki, ADV), (ra...\n",
      "1018    [(lha, INTJ), (ning, PART), (nek, SCONJ), (bap...\n",
      "1019    [(sampun, AUX), (damel, VERB), (ibu, NOUN), (k...\n",
      "Name: train_data, Length: 1020, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# # Data yang akan digunakan untuk training, tanpa 'PUNCT' dan 'SYM'\n",
    "# def remove_punct_sym(kalimat):\n",
    "#     return [(kata, tag) for kata, tag in kalimat if tag not in ['PUNCT', 'SYM']]\n",
    "\n",
    "# # Menerapkan fungsi remove_punct_sym\n",
    "# split_df['train_data'] = split_df['kata_tag'].apply(remove_punct_sym)\n",
    "# print(split_df['train_data'])\n",
    "\n",
    "split_df['train_data'] = split_df['kata_tag']\n",
    "print(split_df['train_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah tag 'PUNCT' dari train data: 2233\n",
      "Jumlah tag 'SYM' dari train data: 12\n"
     ]
    }
   ],
   "source": [
    "# Menghitung jumlah tag 'PUNCT' dan 'SYM' dari train data\n",
    "punct_count = split_df['train_data'].apply(lambda kalimat: sum(1 for kata, tag in kalimat if tag == 'PUNCT')).sum()\n",
    "sym_count = split_df['train_data'].apply(lambda kalimat: sum(1 for kata, tag in kalimat if tag == 'SYM')).sum()\n",
    "\n",
    "print(f\"Jumlah tag 'PUNCT' dari train data: {punct_count}\")\n",
    "print(f\"Jumlah tag 'SYM' dari train data: {sym_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [(bocah, NOUN), (kuwi, DET), (seneng, VERB), (...\n",
      "1       [(ing, ADP), (wayah, NOUN), (sore, NOUN), (bia...\n",
      "2       [(mbakyu, NOUN), (e, PRON), (seneng, VERB), (n...\n",
      "3       [(ibu, NOUN), (kuwi, DET), (ngendikakake, VERB...\n",
      "4       [(nonton, VERB), (tivi, NOUN), (bisa, AUX), (n...\n",
      "                              ...                        \n",
      "1015    [(kanthi, ADP), (rasa, NOUN), (bungah, ADJ), (...\n",
      "1016    [(, PUNCT), (kados, ADV), (dereng, ADV), (sage...\n",
      "1017    [(bocah, NOUN), (enom, ADJ), (saiki, ADV), (ra...\n",
      "1018    [(lha, INTJ), (ning, PART), (nek, SCONJ), (bap...\n",
      "1019    [(sampun, AUX), (damel, VERB), (ibu, NOUN), (k...\n",
      "Name: test_data, Length: 1020, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Testing data tetap mempertahankan semua kata dan tag untuk prediksi otomatis\n",
    "split_df['test_data'] = split_df['kata_tag']\n",
    "print(split_df['test_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghitung probabilitas transisi awal\n",
    "def hitung_probabilitas_awal(sentences):\n",
    "    total_probabilitas_awal = Counter([kalimat[0][1] for kalimat in sentences if kalimat])\n",
    "    total_sentences = len(sentences)\n",
    "    transisi_awal = {pos: freq / total_sentences for pos, freq in total_probabilitas_awal.items()}\n",
    "    return transisi_awal\n",
    "\n",
    "# Fungsi untuk menghitung probabilitas emisi dengan Laplace smoothing\n",
    "def hitung_probabilitas_emisi(word, pos, kata_pos_freq, pos_freq, vocab_size):\n",
    "    kata_pos_count = kata_pos_freq.get((word, pos), 0)\n",
    "    # if kata_pos_count == 0:\n",
    "    #     return 1 / (pos_freq[pos] + vocab_size)\n",
    "    return (kata_pos_count + 1) / (pos_freq[pos] + vocab_size)\n",
    "\n",
    "def hitung_probabilitas_emisi_kata_kosong(pos, pos_freq, vocab_size):\n",
    "    return 1 / (pos_freq[pos] + vocab_size)\n",
    "\n",
    "def hitung_probabilitas_transisi(pos1, pos2, pos_sequence, pos_freq, vocab_size):\n",
    "    pos_bigram_freq = Counter()\n",
    "    pos_bigram_freq.update(zip(pos_sequence, pos_sequence[1:]))\n",
    "    return (pos_bigram_freq[(pos1, pos2)] + 1) / (pos_freq[pos1] + vocab_size)\n",
    "\n",
    "# # Algoritma Forward\n",
    "# def algoritma_forward(sentence, probabilitas_emisi, probabilitas_transisi, probabilitas_awal, pos_tags):\n",
    "#     alpha = [{}]\n",
    "#     for pos in pos_tags:\n",
    "#         alpha[0][pos] = probabilitas_awal.get(pos, 0) * probabilitas_emisi.get((sentence[0], pos), 0)\n",
    "    \n",
    "#     for t in range(1, len(sentence)):\n",
    "#         alpha.append({})\n",
    "#         for pos in pos_tags:\n",
    "#             alpha[t][pos] = sum(alpha[t-1][prev_pos] * probabilitas_transisi.get((prev_pos, pos), 0) * probabilitas_emisi.get((sentence[t], pos), 0) for prev_pos in pos_tags)\n",
    "    \n",
    "#     return alpha\n",
    "\n",
    "#     # Algoritma Backward\n",
    "# def algoritma_backward(sentence, probabilitas_emisi, probabilitas_transisi, pos_tags):\n",
    "#     beta = [{} for _ in range(len(sentence))]\n",
    "    \n",
    "#     # Inisialisasi beta pada waktu t = T\n",
    "#     for pos in pos_tags:\n",
    "#         beta[-1][pos] = 1\n",
    "    \n",
    "#     # Iterasi mundur dari t = T-1 ke t = 0\n",
    "#     for t in range(len(sentence) - 2, -1, -1):\n",
    "#         for pos in pos_tags:\n",
    "#             beta[t][pos] = sum(beta[t + 1][next_pos] * probabilitas_transisi.get((pos, next_pos), 0) * probabilitas_emisi.get((sentence[t + 1], next_pos), 0) for next_pos in pos_tags)\n",
    "    \n",
    "#     return beta\n",
    "\n",
    "# def expectation_step(sentence, alpha, beta, probabilitas_emisi, probabilitas_transisi, pos_tags, pos_freq, vocab_size):\n",
    "#         T = len(sentence)\n",
    "#         gamma = [{} for _ in range(T)]\n",
    "#         ksi = [{} for _ in range(T - 1)]\n",
    "        \n",
    "#         # menghitung gamma\n",
    "#         for t in range(T):\n",
    "#             normalization_factor = sum(alpha[t][pos] * beta[t][pos] for pos in pos_tags)\n",
    "#             for pos in pos_tags:\n",
    "#                 gamma[t][pos] = (alpha[t][pos] * beta[t][pos]) / normalization_factor\n",
    "        \n",
    "#         # menghitung ksi\n",
    "#         for t in range(T - 1):\n",
    "#             normalization_factor = sum(\n",
    "#                 alpha[t][pos1] * probabilitas_transisi.get((pos1, pos2), (1 / (pos_freq[pos1] + vocab_size))) *\n",
    "#                 probabilitas_emisi.get((sentence[t + 1], pos2), (1 / (pos_freq[pos2] + vocab_size))) * beta[t + 1][pos2]\n",
    "#                 for pos1 in pos_tags for pos2 in pos_tags\n",
    "#             )\n",
    "#             for pos1 in pos_tags:\n",
    "#                 ksi[t][pos1] = {}\n",
    "#                 for pos2 in pos_tags:\n",
    "#                     ksi[t][pos1][pos2] = (\n",
    "#                         alpha[t][pos1] * probabilitas_transisi.get((pos1, pos2), (1 / (pos_freq[pos1] + vocab_size))) *\n",
    "#                         probabilitas_emisi.get((sentence[t + 1], pos2), (1 / (pos_freq[pos2] + vocab_size))) * beta[t + 1][pos2]\n",
    "#                     ) / normalization_factor\n",
    "        \n",
    "#         return gamma, ksi\n",
    "\n",
    "# def maximization_step(gamma, ksi, sentence, pos_tags):\n",
    "#     probabilitas_awal_baru = {pos: gamma[0][pos] for pos in pos_tags}\n",
    "    \n",
    "#     probabilitas_transisi_baru = {}\n",
    "#     for pos1 in pos_tags:\n",
    "#         for pos2 in pos_tags:\n",
    "#             a = sum(ksi[t][pos1][pos2] for t in range(len(ksi)))\n",
    "#             b = sum(gamma[t][pos1] for t in range(len(gamma)))\n",
    "#             probabilitas_transisi_baru[(pos1, pos2)] = a / b\n",
    "    \n",
    "#     probabilitas_emisi_baru = {}\n",
    "#     for pos in pos_tags:\n",
    "#         probabilitas_emisi_baru[pos] = {}\n",
    "#         for word in sentence:\n",
    "#             a = sum(gamma[t][pos] for t in range(len(gamma)) if sentence[t] == word)\n",
    "#             b = sum(gamma[t][pos] for t in range(len(gamma)))\n",
    "#             probabilitas_emisi_baru[pos][word] = a / b\n",
    "    \n",
    "#     return probabilitas_awal_baru, probabilitas_transisi_baru, probabilitas_emisi_baru\n",
    "\n",
    "# def predict_tags(gamma):\n",
    "#         return [max(gamma[t], key=gamma[t].get) for t in range(len(gamma))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Forward Algorithm dengan Maximization Step\n",
    "# def algoritma_forward_max(sentence, probabilitas_awal, probabilitas_transisi, probabilitas_emisi, pos_tags):\n",
    "#     # Inisialisasi alpha\n",
    "#     alpha = [{}]\n",
    "#     for pos in pos_tags:\n",
    "#         alpha[0][pos] = probabilitas_awal[pos] * probabilitas_emisi[pos].get(sentence[0], 0)\n",
    "        \n",
    "#     # Iterasi untuk menghitung alpha\n",
    "#     for t in range(1, len(sentence)):\n",
    "#         alpha.append({})\n",
    "#         for pos2 in pos_tags:\n",
    "#             alpha[t][pos2] = sum(alpha[t-1][pos1] * probabilitas_transisi.get((pos1, pos2), 0) * probabilitas_emisi[pos2].get(sentence[t], 0) for pos1 in pos_tags)\n",
    "            \n",
    "#     return alpha\n",
    "\n",
    "# # Backward Algorithm dengan Maximization Step\n",
    "# def algoritma_backward_max(sentence, probabilitas_transisi, probabilitas_emisi, pos_tags):\n",
    "#     # Inisialisasi beta\n",
    "#     beta = [{} for _ in range(len(sentence))]\n",
    "#     for pos in pos_tags:\n",
    "#         beta[len(sentence)-1][pos] = 1\n",
    "        \n",
    "#     # Iterasi untuk menghitung beta\n",
    "#     for t in range(len(sentence)-2, -1, -1):\n",
    "#         for pos1 in pos_tags:\n",
    "#             beta[t][pos1] = sum(probabilitas_transisi.get((pos1, pos2), 0) * probabilitas_emisi[pos2].get(sentence[t+1], 0) * beta[t+1][pos2] for pos2 in pos_tags)\n",
    "            \n",
    "#     return beta\n",
    "\n",
    "# # Expectation Step setelah Maximization: Menghitung gamma dan ksi\n",
    "# def expectation_step_max(sentence, alpha, beta, probabilitas_transisi, probabilitas_emisi, pos_tags):\n",
    "#     gamma = [{} for _ in range(len(alpha))]\n",
    "#     ksi = [{} for _ in range(len(sentence) - 1)]\n",
    "    \n",
    "#     for t in range(len(alpha)):\n",
    "#         normalization_factor = sum(alpha[t][pos] * beta[t][pos] for pos in pos_tags)\n",
    "#         for pos in pos_tags:\n",
    "#             gamma[t][pos] = (alpha[t][pos] * beta[t][pos]) / normalization_factor\n",
    "    \n",
    "#     for t in range(len(sentence) - 1):\n",
    "#         normalization_factor = sum(alpha[t][pos1] * probabilitas_transisi.get((pos1, pos2), 0) * probabilitas_emisi[pos2].get(sentence[t+1], 0) * \n",
    "#                         beta[t+1][pos2] for pos1 in pos_tags for pos2 in pos_tags)\n",
    "#         for pos1 in pos_tags:\n",
    "#             ksi[t][pos1] = {}\n",
    "#             for pos2 in pos_tags:\n",
    "#                 ksi[t][pos1][pos2] = (alpha[t][pos1] * probabilitas_transisi.get((pos1, pos2), 0) * probabilitas_emisi[pos2].get(sentence[t+1], 0) * \n",
    "#                                         beta[t+1][pos2]) / normalization_factor\n",
    "    \n",
    "#     return gamma, ksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "# Menggunakan seluruh dataset sebagai data training\n",
    "train_sentences = split_df['train_data']\n",
    "\n",
    "# Menghitung frekuensi tag POS\n",
    "pos_freq = Counter([tag for kalimat in train_sentences for kata, tag in kalimat])\n",
    "\n",
    "# Menghitung frekuensi pasangan kata-tag POS\n",
    "kata_pos_freq = Counter([(kata, tag) for kalimat in train_sentences for kata, tag in kalimat])\n",
    "\n",
    "pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'SCONJ', 'VERB', 'X']\n",
    "vocab_size = len(pos_tags)  # Jumlah jenis kata\n",
    "\n",
    "# Menghitung probabilitas transisi awal\n",
    "probabilitas_awal = hitung_probabilitas_awal(train_sentences)\n",
    "\n",
    "# Menghitung urutan POS dari data\n",
    "pos_sequence = [tag for sentence in train_sentences for word, tag in sentence]\n",
    "\n",
    "# Menghitung probabilitas transisi untuk setiap pasangan POS\n",
    "probabilitas_transisi = {(pos1, pos2): hitung_probabilitas_transisi(pos1, pos2, pos_sequence, pos_freq, vocab_size) for pos1 in pos_tags for pos2 in pos_tags}\n",
    "\n",
    "# Menghitung probabilitas emisi untuk seluruh kata di training\n",
    "probabilitas_emisi = {}\n",
    "for kata, pos in kata_pos_freq:\n",
    "    probabilitas_emisi[(kata, pos)] = hitung_probabilitas_emisi(kata, pos, kata_pos_freq, pos_freq, vocab_size)\n",
    "\n",
    "probabilitas_emisi_kata_kosong = {}\n",
    "for pos in pos_tags:\n",
    "    probabilitas_emisi_kata_kosong[pos] = hitung_probabilitas_emisi_kata_kosong(pos, pos_freq, vocab_size)\n",
    "\n",
    "# Menyimpan probabilitas ke dalam model\n",
    "model = {\n",
    "    'probabilitas_awal': probabilitas_awal,\n",
    "    'probabilitas_transisi': probabilitas_transisi,\n",
    "    'probabilitas_emisi': probabilitas_emisi,\n",
    "    'vocab_size': vocab_size,\n",
    "    'pos_freq': pos_freq,\n",
    "    'probabilitas_emisi_kata_kosong': probabilitas_emisi_kata_kosong\n",
    "}\n",
    "\n",
    "# Menyimpan model ke file pickle\n",
    "with open('model_Baum_Welch.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kode di bawah enggak fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m root\u001b[38;5;241m.\u001b[39mwithdraw()  \u001b[38;5;66;03m# Menyembunyikan jendela utama\u001b[39;00m\n\u001b[0;32m     64\u001b[0m predict_pos_tagging()\n\u001b[1;32m---> 65\u001b[0m \u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ardian\\anaconda3\\lib\\tkinter\\__init__.py:1429\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmainloop\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "from collections import Counter\n",
    "\n",
    "# Menggunakan seluruh dataset sebagai data training\n",
    "train_sentences = split_df['train_data']\n",
    "\n",
    "# Menghitung frekuensi tag POS\n",
    "pos_freq = Counter([tag for kalimat in train_sentences for kata, tag in kalimat])\n",
    "\n",
    "# Menghitung frekuensi pasangan kata-tag POS\n",
    "kata_pos_freq = Counter([(kata, tag) for kalimat in train_sentences for kata, tag in kalimat])\n",
    "\n",
    "pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'SCONJ', 'VERB', 'X']\n",
    "vocab_size = len(pos_tags)  # Jumlah jenis kata\n",
    "\n",
    "# Menghitung probabilitas transisi awal\n",
    "probabilitas_awal = hitung_probabilitas_awal(train_sentences)\n",
    "\n",
    "# Menghitung urutan POS dari data\n",
    "pos_sequence = [tag for sentence in train_sentences for word, tag in sentence]\n",
    "\n",
    "# Menghitung probabilitas transisi untuk setiap pasangan POS\n",
    "probabilitas_transisi = {(pos1, pos2): hitung_probabilitas_transisi(pos1, pos2, pos_sequence, pos_freq, vocab_size) for pos1 in pos_tags for pos2 in pos_tags}\n",
    "\n",
    "# Membuat GUI untuk input kalimat testing\n",
    "def predict_pos_tagging():\n",
    "    kalimat_testing = simpledialog.askstring(\"Input\", \"Masukkan kalimat untuk prediksi POS tagging:\")\n",
    "    if kalimat_testing:\n",
    "        kalimat_testing = kalimat_testing.lower()\n",
    "        test_sentences = [[(kata, '') for kata in kalimat_testing.split()]]\n",
    "        probabilitas_emisi = {}\n",
    "        for kalimat in test_sentences:\n",
    "            for kata, _ in kalimat:\n",
    "                for pos in pos_tags:\n",
    "                    probabilitas_emisi[(kata, pos)] = hitung_probabilitas_emisi(kata, pos, kata_pos_freq, pos_freq, vocab_size)\n",
    "\n",
    "        for kalimat in test_sentences:\n",
    "            sentence = [kata for kata, tag in kalimat]\n",
    "            alpha = algoritma_forward(sentence, probabilitas_emisi, probabilitas_transisi, probabilitas_awal, pos_tags)\n",
    "            beta = algoritma_backward(sentence, probabilitas_emisi, probabilitas_transisi, pos_tags)\n",
    "            gamma, ksi = expectation_step(sentence, alpha, beta, probabilitas_emisi, probabilitas_transisi, pos_tags, pos_freq, vocab_size)\n",
    "            probabilitas_awal_baru, probabilitas_transisi_baru, probabilitas_emisi_baru = maximization_step(gamma, ksi, sentence, pos_tags)\n",
    "            alpha_max = algoritma_forward_max(sentence, probabilitas_awal_baru, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "            beta_max = algoritma_backward_max(sentence, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "            likelihood = sum(alpha[-1][pos] * beta_max[-1][pos] for pos in pos_tags)\n",
    "            gamma_max, ksi_max = expectation_step_max(sentence, alpha_max, beta_max, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "\n",
    "            # Menampilkan hasil prediksi POS tagging\n",
    "            hasil_prediksi = []\n",
    "            for t, kata in enumerate(sentence):\n",
    "                if kata in ['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '-']:\n",
    "                    pos_prediksi = 'PUNCT'\n",
    "                elif kata in ['$', '%', '@', '&', '#', '*']:\n",
    "                    pos_prediksi = 'SYM'\n",
    "                else:\n",
    "                    pos_prediksi = max(gamma_max[t], key=gamma_max[t].get)\n",
    "                hasil_prediksi.append((kata, pos_prediksi))\n",
    "            messagebox.showinfo(\"Hasil Prediksi POS Tagging\", hasil_prediksi)\n",
    "\n",
    "# Membuat GUI\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Menyembunyikan jendela utama\n",
    "predict_pos_tagging()\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpan Model Menggunakan Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "# Menggunakan seluruh dataset sebagai data training\n",
    "train_sentences = split_df['train_data']\n",
    "\n",
    "# Menghitung frekuensi tag POS\n",
    "pos_freq = Counter([tag for kalimat in train_sentences for kata, tag in kalimat])\n",
    "\n",
    "# Menghitung frekuensi pasangan kata-tag POS\n",
    "kata_pos_freq = Counter([(kata, tag) for kalimat in train_sentences for kata, tag in kalimat])\n",
    "\n",
    "pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'SCONJ', 'VERB', 'X']\n",
    "vocab_size = len(pos_tags)  # Jumlah jenis kata\n",
    "\n",
    "# Menghitung probabilitas transisi awal\n",
    "probabilitas_awal = hitung_probabilitas_awal(train_sentences)\n",
    "\n",
    "# Menghitung urutan POS dari data\n",
    "pos_sequence = [tag for sentence in train_sentences for word, tag in sentence]\n",
    "\n",
    "# Menghitung probabilitas transisi untuk setiap pasangan POS\n",
    "probabilitas_transisi = {(pos1, pos2): hitung_probabilitas_transisi(pos1, pos2, pos_sequence, pos_freq, vocab_size) for pos1 in pos_tags for pos2 in pos_tags}\n",
    "\n",
    "# Menggunakan seluruh dataset sebagai kalimat testing\n",
    "test_sentences = split_df['test_data']\n",
    "\n",
    "# Menyimpan hasil model menggunakan pickle\n",
    "model_results = []\n",
    "\n",
    "for kalimat in test_sentences:\n",
    "    # sentence = list(dict.fromkeys([kata for kata, tag in kalimat]))  # Menghapus kata duplikat\n",
    "    sentence = [kata for kata, tag in kalimat]  # Menggunakan semua kata dalam kalimat\n",
    "    probabilitas_emisi = {}\n",
    "    for kata, _ in kalimat:\n",
    "        for pos in pos_tags:\n",
    "            probabilitas_emisi[(kata, pos)] = hitung_probabilitas_emisi(kata, pos, kata_pos_freq, pos_freq, vocab_size)\n",
    "\n",
    "    alpha = algoritma_forward(sentence, probabilitas_emisi, probabilitas_transisi, probabilitas_awal, pos_tags)\n",
    "    beta = algoritma_backward(sentence, probabilitas_emisi, probabilitas_transisi, pos_tags)\n",
    "    gamma, ksi = expectation_step(sentence, alpha, beta, probabilitas_emisi, probabilitas_transisi, pos_tags, pos_freq, vocab_size)\n",
    "    probabilitas_awal_baru, probabilitas_transisi_baru, probabilitas_emisi_baru = maximization_step(gamma, ksi, sentence, pos_tags)\n",
    "    alpha_max = algoritma_forward_max(sentence, probabilitas_awal_baru, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "    beta_max = algoritma_backward_max(sentence, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "    likelihood = sum(alpha[-1][pos] * beta_max[-1][pos] for pos in pos_tags)\n",
    "    gamma_max, ksi_max = expectation_step_max(sentence, alpha_max, beta_max, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "\n",
    "    # Menyimpan hasil prediksi POS tagging\n",
    "    hasil_prediksi = []\n",
    "    for t, kata in enumerate(sentence):\n",
    "        if kata in ['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '-']:\n",
    "            pos_prediksi = 'PUNCT'\n",
    "        elif kata in ['$', '%', '@', '&', '#', '*']:\n",
    "            pos_prediksi = 'SYM'\n",
    "        else:\n",
    "            pos_prediksi = max(gamma_max[t], key=gamma_max[t].get)\n",
    "        hasil_prediksi.append((kata, pos_prediksi))\n",
    "    \n",
    "    model_results.append(hasil_prediksi)\n",
    "\n",
    "# Menyimpan hasil model ke file pickle\n",
    "with open('model_results.pkl', 'wb') as file:\n",
    "    pickle.dump(model_results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpan Model Menggunakan Pickle kedua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "# Menggunakan seluruh dataset sebagai data training\n",
    "train_sentences = split_df['train_data']\n",
    "\n",
    "# Menghitung frekuensi tag POS\n",
    "pos_freq = Counter([tag for kalimat in train_sentences for kata, tag in kalimat])\n",
    "\n",
    "# Menghitung frekuensi pasangan kata-tag POS\n",
    "kata_pos_freq = Counter([(kata, tag) for kalimat in train_sentences for kata, tag in kalimat])\n",
    "\n",
    "pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'SCONJ', 'VERB', 'X']\n",
    "vocab_size = len(pos_tags)  # Jumlah jenis kata\n",
    "\n",
    "# Menghitung probabilitas transisi awal\n",
    "probabilitas_awal = hitung_probabilitas_awal(train_sentences)\n",
    "\n",
    "# Menghitung urutan POS dari data\n",
    "pos_sequence = [tag for sentence in train_sentences for word, tag in sentence]\n",
    "\n",
    "# Menghitung probabilitas transisi untuk setiap pasangan POS\n",
    "probabilitas_transisi = {(pos1, pos2): hitung_probabilitas_transisi(pos1, pos2, pos_sequence, pos_freq, vocab_size) for pos1 in pos_tags for pos2 in pos_tags}\n",
    "\n",
    "# Menghitung probabilitas emisi untuk seluruh kata di training\n",
    "probabilitas_emisi = {}\n",
    "for kata, pos in kata_pos_freq:\n",
    "    probabilitas_emisi[(kata, pos)] = hitung_probabilitas_emisi(kata, pos, kata_pos_freq, pos_freq, vocab_size)\n",
    "\n",
    "probabilitas_emisi_kata_kosong = {}\n",
    "for pos in pos_tags:\n",
    "    probabilitas_emisi_kata_kosong[pos] = hitung_probabilitas_emisi_kata_kosong(pos, pos_freq, vocab_size)\n",
    "\n",
    "# Menyimpan probabilitas ke dalam model\n",
    "model = {\n",
    "    'probabilitas_awal': probabilitas_awal,\n",
    "    'probabilitas_transisi': probabilitas_transisi,\n",
    "    'probabilitas_emisi': probabilitas_emisi,\n",
    "    'vocab_size': vocab_size,\n",
    "    'pos_freq': pos_freq,\n",
    "    'probabilitas_emisi_kata_kosong': probabilitas_emisi_kata_kosong\n",
    "}\n",
    "\n",
    "# Menyimpan model ke file pickle\n",
    "with open('model_Baum_Welch.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model Menggunakan GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "import pickle\n",
    "\n",
    "# Load the model results from the pickle file\n",
    "with open('model_results.pkl', 'rb') as file:\n",
    "    model_results = pickle.load(file)\n",
    "\n",
    "# Define the POS tags\n",
    "pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'SCONJ', 'VERB', 'X']\n",
    "\n",
    "# Function to clear the frame\n",
    "def clear_frame():\n",
    "    for widget in root.winfo_children():\n",
    "        widget.destroy()\n",
    "\n",
    "# Function to predict POS tags for a new sentence\n",
    "def predict_pos_tagging():\n",
    "    clear_frame()\n",
    "\n",
    "    # Create input prompt\n",
    "    prompt_label = tk.Label(root, text=\"Masukkan kalimat untuk prediksi POS tagging:\")\n",
    "    prompt_label.pack(pady=10)\n",
    "\n",
    "    # Input field\n",
    "    input_field = tk.Entry(root, width=50)\n",
    "    input_field.pack(pady=10)\n",
    "\n",
    "    # Result label\n",
    "    result_label = tk.Label(root, text=\"\", justify=\"left\")\n",
    "    result_label.pack(pady=10)\n",
    "\n",
    "    # Function to process the input\n",
    "    def process_input():\n",
    "        kalimat_testing = input_field.get()\n",
    "        if kalimat_testing:\n",
    "            kalimat_testing = kalimat_testing.lower()\n",
    "            test_sentence = kalimat_testing.split()\n",
    "\n",
    "            # Aggregate results from all sentences in the model results\n",
    "            hasil_Keseluruhan = []\n",
    "            for result in model_results:\n",
    "                for word, tag in result:\n",
    "                    if word in test_sentence:\n",
    "                        hasil_Keseluruhan.append((word, tag))\n",
    "\n",
    "            # Remove duplicates while preserving order\n",
    "            dilihat = set()\n",
    "            hasil_prediksi = [(word, tag) for word, tag in hasil_Keseluruhan if not (word in dilihat or dilihat.add(word))]\n",
    "\n",
    "            # Display the predicted POS tags\n",
    "            if hasil_prediksi:\n",
    "                result_text = \"\\n\".join(f\"{word}: {tag}\" for word, tag in hasil_prediksi)\n",
    "                result_label.config(text=f\"Hasil Prediksi: {result_text}\")\n",
    "                \n",
    "                # Check for words that could not be predicted\n",
    "                kata_prediksi = {word for word, tag in hasil_prediksi}\n",
    "                kata_baru = [word for word in test_sentence if word not in kata_prediksi]\n",
    "                if kata_baru:\n",
    "                    missing_text = \", \".join(word for word in kata_baru)\n",
    "                    result_label.config(text=f\"Hasil Prediksi: {result_text}\\n\\nKata yang tidak bisa diprediksi:\\n{missing_text}\")\n",
    "            else:\n",
    "                result_label.config(text=\"Tidak ada kata yang ditemukan di kalimat input.\")\n",
    "\n",
    "    # Predict button\n",
    "    predict_button = tk.Button(root, text=\"Predict\", command=process_input)\n",
    "    predict_button.pack(pady=5)\n",
    "\n",
    "    # New button to restart the prediction\n",
    "    def new_prediction():\n",
    "        predict_pos_tagging()\n",
    "\n",
    "    new_button = tk.Button(root, text=\"New\", command=new_prediction)\n",
    "    new_button.pack(pady=5)\n",
    "\n",
    "# Create the GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"POS Tagging Predictor\")\n",
    "\n",
    "predict_pos_tagging()\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kode Dibawah Masih Salah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "import pickle\n",
    "\n",
    "# Load the model results from the pickle file\n",
    "with open('model_results.pkl', 'rb') as file:\n",
    "    model_results = pickle.load(file)\n",
    "\n",
    "# Define the POS tags\n",
    "pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'SCONJ', 'VERB', 'X']\n",
    "\n",
    "# Function to clear the frame\n",
    "def clear_frame():\n",
    "    for widget in root.winfo_children():\n",
    "        widget.destroy()\n",
    "\n",
    "# Function to predict POS tags for a new sentence\n",
    "def predict_pos_tagging():\n",
    "    clear_frame()\n",
    "\n",
    "    # Create input prompt\n",
    "    prompt_label = tk.Label(root, text=\"Masukkan kalimat untuk prediksi POS tagging:\")\n",
    "    prompt_label.pack(pady=10)\n",
    "\n",
    "    # Input field\n",
    "    input_field = tk.Entry(root, width=50)\n",
    "    input_field.pack(pady=10)\n",
    "\n",
    "    # Result label\n",
    "    result_label = tk.Label(root, text=\"\", justify=\"left\")\n",
    "    result_label.pack(pady=10)\n",
    "\n",
    "    # Function to process the input\n",
    "    def process_input():\n",
    "        kalimat_testing = input_field.get()\n",
    "        if kalimat_testing:\n",
    "            kalimat_testing = kalimat_testing.lower()\n",
    "            test_sentence = kalimat_testing.split()\n",
    "\n",
    "            # Find the most similar sentence in the model results\n",
    "            best_match = None\n",
    "            best_score = float('-inf')  # Use -inf for maximizing score\n",
    "            for result in model_results:\n",
    "                score = sum(1 for word, _ in result if word in test_sentence)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_match = result\n",
    "\n",
    "            # Display the predicted POS tags\n",
    "            if best_match:\n",
    "                hasil_prediksi = [(word, tag) for word, tag in best_match if word in test_sentence]\n",
    "                if hasil_prediksi:\n",
    "                    result_text = \"\\n\".join(f\"{word}: {tag}\" for word, tag in hasil_prediksi)\n",
    "                    result_label.config(text=f\"Hasil Prediksi: {result_text}\")\n",
    "                else:\n",
    "                    result_label.config(text=\"Tidak ada kata yang ditemukan di kalimat input.\")\n",
    "            else:\n",
    "                result_label.config(text=\"Tidak ada prediksi yang cocok ditemukan.\")\n",
    "\n",
    "    # Predict button\n",
    "    predict_button = tk.Button(root, text=\"Predict\", command=process_input)\n",
    "    predict_button.pack(pady=5)\n",
    "\n",
    "    # New button to restart the prediction\n",
    "    def new_prediction():\n",
    "        predict_pos_tagging()\n",
    "\n",
    "    new_button = tk.Button(root, text=\"New\", command=new_prediction)\n",
    "    new_button.pack(pady=5)\n",
    "\n",
    "# Create the GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"POS Tagging Predictor\")\n",
    "\n",
    "predict_pos_tagging()\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kode Dibawah Masih Salah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "# Menggunakan seluruh dataset sebagai data training\n",
    "train_sentences = split_df['train_data']\n",
    "\n",
    "# Menghitung frekuensi tag POS\n",
    "pos_freq = Counter([tag for kalimat in train_sentences for kata, tag in kalimat])\n",
    "\n",
    "# Menghitung frekuensi pasangan kata-tag POS\n",
    "kata_pos_freq = Counter([(kata, tag) for kalimat in train_sentences for kata, tag in kalimat])\n",
    "\n",
    "pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'SCONJ', 'VERB', 'X']\n",
    "vocab_size = len(pos_tags)  # Jumlah jenis kata\n",
    "\n",
    "# Menghitung probabilitas transisi awal\n",
    "probabilitas_awal = hitung_probabilitas_awal(train_sentences)\n",
    "\n",
    "# Menghitung urutan POS dari data\n",
    "pos_sequence = [tag for sentence in train_sentences for word, tag in sentence]\n",
    "\n",
    "# Menghitung probabilitas transisi untuk setiap pasangan POS\n",
    "probabilitas_transisi = {(pos1, pos2): hitung_probabilitas_transisi(pos1, pos2, pos_sequence, pos_freq, vocab_size) for pos1 in pos_tags for pos2 in pos_tags}\n",
    "\n",
    "# Menggunakan seluruh dataset sebagai kalimat testing\n",
    "test_sentences = split_df['test_data']\n",
    "\n",
    "# Menyimpan hasil model menggunakan pickle\n",
    "model_results = []\n",
    "\n",
    "for kalimat in test_sentences:\n",
    "    sentence = [kata for kata, tag in kalimat]\n",
    "    kalimat_testing = ' '.join(sentence)\n",
    "    if kalimat_testing:\n",
    "        kalimat_testing = kalimat_testing.lower()\n",
    "        test_sentences = [[(kata, '') for kata in kalimat_testing.split()]]\n",
    "        probabilitas_emisi = {}\n",
    "        for kalimat in test_sentences:\n",
    "            for kata, _ in kalimat:\n",
    "                for pos in pos_tags:\n",
    "                    probabilitas_emisi[(kata, pos)] = hitung_probabilitas_emisi(kata, pos, kata_pos_freq, pos_freq, vocab_size)\n",
    "\n",
    "        for kalimat in test_sentences:\n",
    "            sentence = [kata for kata, tag in kalimat]\n",
    "            alpha = algoritma_forward(sentence, probabilitas_emisi, probabilitas_transisi, probabilitas_awal, pos_tags)\n",
    "            beta = algoritma_backward(sentence, probabilitas_emisi, probabilitas_transisi, pos_tags)\n",
    "            gamma, ksi = expectation_step(sentence, alpha, beta, probabilitas_emisi, probabilitas_transisi, pos_tags, pos_freq, vocab_size)\n",
    "            probabilitas_awal_baru, probabilitas_transisi_baru, probabilitas_emisi_baru = maximization_step(gamma, ksi, sentence, pos_tags)\n",
    "            alpha_max = algoritma_forward_max(sentence, probabilitas_awal_baru, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "            beta_max = algoritma_backward_max(sentence, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "            likelihood = sum(alpha[-1][pos] * beta_max[-1][pos] for pos in pos_tags)\n",
    "            gamma_max, ksi_max = expectation_step_max(sentence, alpha_max, beta_max, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "\n",
    "            # Menyimpan hasil prediksi POS tagging\n",
    "            hasil_prediksi = []\n",
    "            for t, kata in enumerate(sentence):\n",
    "                if kata in ['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '-']:\n",
    "                    pos_prediksi = 'PUNCT'\n",
    "                elif kata in ['$', '%', '@', '&', '#', '*']:\n",
    "                    pos_prediksi = 'SYM'\n",
    "                else:\n",
    "                    pos_prediksi = max(gamma_max[t], key=gamma_max[t].get)\n",
    "                hasil_prediksi.append((kata, pos_prediksi))\n",
    "    \n",
    "            model_results.append(hasil_prediksi)\n",
    "\n",
    "# Menyimpan hasil model ke file pickle\n",
    "with open('model_results_Baru.pkl', 'wb') as file:\n",
    "    pickle.dump(model_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "import pickle\n",
    "\n",
    "# Load the model results from the pickle file\n",
    "with open('model_results_Baru.pkl', 'rb') as file:\n",
    "    model_results = pickle.load(file)\n",
    "\n",
    "# Define the POS tags\n",
    "pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'SCONJ', 'VERB', 'X']\n",
    "\n",
    "# Function to clear the frame\n",
    "def clear_frame():\n",
    "    for widget in root.winfo_children():\n",
    "        widget.destroy()\n",
    "\n",
    "# Function to predict POS tags for a new sentence\n",
    "def predict_pos_tagging():\n",
    "    clear_frame()\n",
    "\n",
    "    # Create input prompt\n",
    "    prompt_label = tk.Label(root, text=\"Masukkan kalimat untuk prediksi POS tagging:\")\n",
    "    prompt_label.pack(pady=10)\n",
    "\n",
    "    # Input field\n",
    "    input_field = tk.Entry(root, width=50)\n",
    "    input_field.pack(pady=10)\n",
    "\n",
    "    # Result label\n",
    "    result_label = tk.Label(root, text=\"\", justify=\"left\")\n",
    "    result_label.pack(pady=10)\n",
    "\n",
    "    # Function to process the input\n",
    "    def process_input():\n",
    "        kalimat_testing = input_field.get()\n",
    "        if kalimat_testing:\n",
    "            kalimat_testing = kalimat_testing.lower()\n",
    "            test_sentence = kalimat_testing.split()\n",
    "\n",
    "            # Find the most similar sentence in the model results\n",
    "            best_match = None\n",
    "            best_score = float('-inf')  # Use -inf for maximizing score\n",
    "            for result in model_results:\n",
    "                score = sum(1 for word, _ in result if word in test_sentence)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_match = result\n",
    "\n",
    "            # Display the predicted POS tags\n",
    "            if best_match:\n",
    "                hasil_prediksi = [(word, tag) for word, tag in best_match if word in test_sentence]\n",
    "                if hasil_prediksi:\n",
    "                    result_text = \"\\n\".join(f\"{word}: {tag}\" for word, tag in hasil_prediksi)\n",
    "                    result_label.config(text=f\"Hasil Prediksi: {result_text}\")\n",
    "                else:\n",
    "                    result_label.config(text=\"Tidak ada kata yang ditemukan di kalimat input.\")\n",
    "            else:\n",
    "                result_label.config(text=\"Tidak ada prediksi yang cocok ditemukan.\")\n",
    "\n",
    "    # Predict button\n",
    "    predict_button = tk.Button(root, text=\"Predict\", command=process_input)\n",
    "    predict_button.pack(pady=5)\n",
    "\n",
    "    # New button to restart the prediction\n",
    "    def new_prediction():\n",
    "        predict_pos_tagging()\n",
    "\n",
    "    new_button = tk.Button(root, text=\"New\", command=new_prediction)\n",
    "    new_button.pack(pady=5)\n",
    "\n",
    "# Create the GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"POS Tagging Predictor\")\n",
    "\n",
    "predict_pos_tagging()\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "# Menggunakan seluruh dataset sebagai data training\n",
    "train_sentences = split_df['train_data']\n",
    "\n",
    "# Menghitung frekuensi tag POS\n",
    "pos_freq = Counter([tag for kalimat in train_sentences for kata, tag in kalimat])\n",
    "\n",
    "# Menghitung frekuensi pasangan kata-tag POS\n",
    "kata_pos_freq = Counter([(kata, tag) for kalimat in train_sentences for kata, tag in kalimat])\n",
    "\n",
    "pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'SCONJ', 'VERB', 'X']\n",
    "vocab_size = len(pos_tags)  # Jumlah jenis kata\n",
    "\n",
    "# Menghitung probabilitas transisi awal\n",
    "probabilitas_awal = hitung_probabilitas_awal(train_sentences)\n",
    "\n",
    "# Menghitung urutan POS dari data\n",
    "pos_sequence = [tag for sentence in train_sentences for word, tag in sentence]\n",
    "\n",
    "# Menghitung probabilitas transisi untuk setiap pasangan POS\n",
    "probabilitas_transisi = {(pos1, pos2): hitung_probabilitas_transisi(pos1, pos2, pos_sequence, pos_freq, vocab_size) for pos1 in pos_tags for pos2 in pos_tags}\n",
    "\n",
    "# Menghitung probabilitas emisi untuk seluruh kata di training\n",
    "probabilitas_emisi = {}\n",
    "for kata, pos in kata_pos_freq:\n",
    "    probabilitas_emisi[(kata, pos)] = hitung_probabilitas_emisi(kata, pos, kata_pos_freq, pos_freq, vocab_size)\n",
    "\n",
    "probabilitas_emisi_kata_kosong = {}\n",
    "for pos in pos_tags:\n",
    "    probabilitas_emisi_kata_kosong[pos] = hitung_probabilitas_emisi_kata_kosong(pos, pos_freq, vocab_size)\n",
    "\n",
    "# Menyimpan probabilitas ke dalam model\n",
    "model = {\n",
    "    'probabilitas_awal': probabilitas_awal,\n",
    "    'probabilitas_transisi': probabilitas_transisi,\n",
    "    'probabilitas_emisi': probabilitas_emisi,\n",
    "    'vocab_size': vocab_size,\n",
    "    'pos_freq': pos_freq,\n",
    "    'probabilitas_emisi_kata_kosong': probabilitas_emisi_kata_kosong\n",
    "}\n",
    "\n",
    "# Menyimpan model ke file pickle\n",
    "with open('model_Baum_Welch.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import simpledialog, messagebox\n",
    "import pickle\n",
    "\n",
    "# Load the model results from the pickle file\n",
    "with open('model_Baum_Welch.pkl', 'rb') as file:\n",
    "    model_results = pickle.load(file)\n",
    "\n",
    "# Define the POS tags\n",
    "pos_tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'SCONJ', 'VERB', 'X']\n",
    "\n",
    "# Function to clear the frame\n",
    "def clear_frame():\n",
    "    for widget in root.winfo_children():\n",
    "        widget.destroy()\n",
    "\n",
    "# Function to predict POS tags for a new sentence\n",
    "def predict_pos_tagging():\n",
    "    clear_frame()\n",
    "\n",
    "    # Create input prompt\n",
    "    prompt_label = tk.Label(root, text=\"Masukkan kalimat untuk prediksi POS tagging:\")\n",
    "    prompt_label.pack(pady=10)\n",
    "\n",
    "    # Input field\n",
    "    input_field = tk.Entry(root, width=50)\n",
    "    input_field.pack(pady=10)\n",
    "\n",
    "    # Result label\n",
    "    result_label = tk.Label(root, text=\"\", justify=\"left\")\n",
    "    result_label.pack(pady=10)\n",
    "\n",
    "    # Function to process the input\n",
    "    def process_input():\n",
    "        kalimat_testing = input_field.get()\n",
    "        if kalimat_testing:\n",
    "            kalimat_testing = kalimat_testing.lower()\n",
    "            test_sentence = kalimat_testing.split()\n",
    "\n",
    "            # Extract probabilities from the model\n",
    "            probabilitas_awal = model_results['probabilitas_awal']\n",
    "            probabilitas_transisi = model_results['probabilitas_transisi']\n",
    "            probabilitas_emisi = model_results['probabilitas_emisi']\n",
    "            vocab_size = model_results['vocab_size']\n",
    "            pos_freq = model_results['pos_freq']\n",
    "            probabilitas_emisi_kata_kosong = model_results['probabilitas_emisi_kata_kosong']\n",
    "\n",
    "            # Process the input sentence\n",
    "            sentence = test_sentence\n",
    "\n",
    "            # Initialize alpha and beta\n",
    "            alpha = algoritma_forward(sentence, probabilitas_emisi, probabilitas_transisi, probabilitas_awal, pos_tags)\n",
    "            beta = algoritma_backward(sentence, probabilitas_emisi, probabilitas_transisi, pos_tags)\n",
    "\n",
    "            # Perform expectation step\n",
    "            gamma, ksi = expectation_step(sentence, alpha, beta, probabilitas_emisi, probabilitas_transisi, pos_tags, pos_freq, vocab_size)\n",
    "\n",
    "            # Perform maximization step\n",
    "            probabilitas_awal_baru, probabilitas_transisi_baru, probabilitas_emisi_baru = maximization_step(gamma, ksi, sentence, pos_tags)\n",
    "\n",
    "            # Initialize alpha_max and beta_max\n",
    "            alpha_max = algoritma_forward_max(sentence, probabilitas_awal_baru, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "            beta_max = algoritma_backward_max(sentence, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "\n",
    "            # Calculate likelihood\n",
    "            likelihood = sum(alpha[-1][pos] * beta_max[-1][pos] for pos in pos_tags)\n",
    "\n",
    "            # Perform expectation step with maximization\n",
    "            gamma_max, ksi_max = expectation_step_max(sentence, alpha_max, beta_max, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "\n",
    "            # Set threshold and initialize variables for iteration\n",
    "            threshold = 1e-10\n",
    "            likelihood_diff = float('inf')\n",
    "            prev_likelihood = 0\n",
    "            iteration = 0\n",
    "\n",
    "            # Iterate until convergence\n",
    "            while likelihood_diff > threshold:\n",
    "                iteration += 1\n",
    "\n",
    "                alpha_max = algoritma_forward_max(sentence, probabilitas_awal_baru, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "                beta_max = algoritma_backward_max(sentence, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "\n",
    "                gamma_max, ksi_max = expectation_step_max(sentence, alpha_max, beta_max, probabilitas_transisi_baru, probabilitas_emisi_baru, pos_tags)\n",
    "\n",
    "                probabilitas_awal_baru, probabilitas_transisi_baru, probabilitas_emisi_baru = maximization_step(gamma_max, ksi_max, sentence, pos_tags)\n",
    "\n",
    "                likelihood = sum(alpha_max[-1][pos] * beta_max[-1][pos] for pos in pos_tags)\n",
    "\n",
    "                current_likelihood = likelihood\n",
    "                if iteration >= 2:\n",
    "                    likelihood_diff = current_likelihood - prev_likelihood\n",
    "                    prev_likelihood = current_likelihood\n",
    "                else:\n",
    "                    prev_likelihood = current_likelihood\n",
    "\n",
    "                print(f\"Iteration {iteration}: Likelihood = {current_likelihood:.9f}, Change in Likelihood = {likelihood_diff:.9f}\")\n",
    "\n",
    "            print(\"\\nFinal Likelihood:\", likelihood)\n",
    "\n",
    "            # Predict POS tags\n",
    "            predicted_tags = []\n",
    "            for t in range(len(sentence)):\n",
    "                kata = sentence[t]\n",
    "                if kata in ['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '-']:\n",
    "                    predicted_tags.append('PUNCT')\n",
    "                elif kata in ['$', '%', '@', '&', '#', '*']:\n",
    "                    predicted_tags.append('SYM')\n",
    "                else:\n",
    "                    predicted_tags.append(max(gamma_max[t], key=gamma_max[t].get))\n",
    "\n",
    "            # Display the predicted POS tags\n",
    "            result_text = \"\\n\".join(f\"{word}: {tag}\" for word, tag in zip(sentence, predicted_tags))\n",
    "            result_label.config(text=f\"Hasil Prediksi: {result_text}\")\n",
    "\n",
    "            \n",
    "\n",
    "    # Predict button\n",
    "    predict_button = tk.Button(root, text=\"Predict\", command=process_input)\n",
    "    predict_button.pack(pady=5)\n",
    "\n",
    "    # New button to restart the prediction\n",
    "    def new_prediction():\n",
    "        predict_pos_tagging()\n",
    "\n",
    "    new_button = tk.Button(root, text=\"New\", command=new_prediction)\n",
    "    new_button.pack(pady=5)\n",
    "\n",
    "# Create the GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"POS Tagging Predictor\")\n",
    "\n",
    "predict_pos_tagging()\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
